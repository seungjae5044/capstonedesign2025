{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9264924f37484e2db321ac8e478c3f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"model/llama-3.2-Korean-Bllossom-3B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/llama-3.2-Korean-Bllossom-3B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End tod end LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompting:\n",
    "    def __init__(self, topic, model_ = \"llama-3.2-Korean-Bllossom-3B\"):\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"model/llama-3.2-Korean-Bllossom-3B\",\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"model/{model_}\")\n",
    "        self.topic = topic\n",
    "        self.system = f'''\n",
    "\n",
    "You are an evaluation model for meeting transcripts. Please follow these guidelines for each utterance in the conversation:\n",
    "\n",
    "1. Topic Coherence (0.0–10.0):\n",
    "   - The topic of this meeting is {self.topic}.\n",
    "   - For each utterance, evaluate how closely it aligns to the meeting topic.\n",
    "   - Provide a numerical score between 0.0 and 10.0, where 0.0 indicates no relevance and 10.0 indicates strong, direct relevance.\n",
    "   - Alongside the score, explain briefly why you assigned that score by korean.\n",
    "\n",
    "2. Repetition Avoidance (0.0–10.0):\n",
    "   - Compare each utterance with mentioned ideas, suggestions, or statements in previously utterances.\n",
    "   - If the content is largely duplicative, assign a lower score. If it adds new perspectives or content, assign a higher score.\n",
    "   - Provide a numerical score between 0.0 and 10.0, along with a short rationale explaining the basis for your judgment by korean.\n",
    "\n",
    "3. Additional Considerations:\n",
    "   - Base your evaluations primarily on these two criteria, maintaining consistency.\n",
    "   - After evaluating all utterances, compile an overall summary or feedback addressing which parts of the conversation showed strong topic coherence and introduced fresh insights, and which parts repeated prior points.\n",
    "\n",
    "Your output for each utterance should follow only this format:\n",
    "\n",
    "- Topic Coherence: <Score> (Rationale: <Reasoning>)\n",
    "- Repetition Avoidance: <Score> (Rationale: <Reasoning>)\n",
    "'''\n",
    "        self.utterances = \"\"\n",
    "\n",
    "    def eval(self, text):\n",
    "        \n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": self.system},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "            {\"role\": \"previous utterances\", \"content\": self.utterances}\n",
    "        ]\n",
    "        \n",
    "        prompt= self.tokenizer.apply_chat_template(chat, tokenize = False, add_generation_prompt=True)\n",
    "\n",
    "        model_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        terminators = [\n",
    "            self.tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "            self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        if \"token_type_ids\" in model_inputs:\n",
    "            del model_inputs[\"token_type_ids\"]\n",
    "        \n",
    "        model_inputs = {k: v.to(model.device) for k, v in model_inputs.items()}\n",
    "        \n",
    "        generated_ids = self.model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        prompt_length = model_inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        result = self.tokenizer.decode(generated_ids[0][prompt_length:], skip_special_tokens=True)\n",
    "        \n",
    "        self.utterances = self.utterances + text +  \"\\n\"\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e218cf16ef419ea979960a8f3e6578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic = \"현대 대한민국 사회는 행복한가?\"\n",
    "model = Prompting(topic=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 음성데이터 -> whishper -> txt 파일 -> LLM: 이문장의 주제를 한문장으로 알려줘 = Topic\n",
    "## topic, txt파일 -> LLM:Evaluation = 평가 결과 (model의 prediction)\n",
    "## txt 파일 -> Humen feedback = 정답지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 4초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=7.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    4초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 4초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 4초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 4초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks + 1):  # 마지막 청크까지 포함\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        \n",
    "        # 마지막 청크 처리 (길이가 chunk_length보다 짧을 수 있음)\n",
    "        if start >= len(audio):\n",
    "            break\n",
    "        if end > len(audio):\n",
    "            end = len(audio)\n",
    "        \n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=4.0, sampling_rate=16000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
