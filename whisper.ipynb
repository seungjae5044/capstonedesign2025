{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a45bf19-cc1c-4821-b9cd-cb40aece79ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  오늘 여자 올려 구경하냐? 뭐 보고 있냐 지금? 지금 이 여자가 보고 라코랑 매종 기치내에서 그... 뭐야, 카라티 하나씩 커플로 사고. 얼마 안 돼? 라코를 두 개에서 15만원 샀고 매종을 두 개에서 20,000만원. 괜찮네 두 개에 2,000이면. 그래서 이렇게 내게 사서 40개 안 들었으니까.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 1. Whisper Tiny 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 3. 오디오 파일 로드 및 처리\n",
    "def load_audio(audio_path, sampling_rate=16000):\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)  # 16kHz로 샘플링\n",
    "    return audio\n",
    "\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "audio = load_audio(audio_path)\n",
    "\n",
    "# 4. 입력 데이터 준비\n",
    "input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "# 5. 모델 추론\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# 6. 결과 디코딩\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# 7. 결과 출력\n",
    "print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a437c17-e512-4976-8986-baf0fff62d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 1.50s]:  Oh\n",
      "[1.50s - 3.00s]:  오늘 여자 올려 부검하냐?\n",
      "[3.00s - 4.50s]:  Oh\n",
      "[4.50s - 6.00s]: 哇,我不过应该这个\n",
      "[6.00s - 7.50s]:  시간은\n",
      "[7.50s - 9.00s]:  그리고 이곳에서\n",
      "[9.00s - 10.50s]:  랩소라운 매주\n",
      "[10.50s - 12.00s]:  좋은 기침나에서\n",
      "[12.00s - 13.50s]:  Mmm, good.\n",
      "[13.50s - 15.00s]: カラッキー話し\n",
      "[15.00s - 16.50s]:  어마어마한 택시\n",
      "[16.50s - 18.00s]:  لقوم\n",
      "[18.00s - 19.50s]:  두 개에서 15만원 샀고\n",
      "[19.50s - 21.00s]:  음\n",
      "[21.00s - 22.50s]:  두 개에서 20천만 원\n",
      "[22.50s - 24.00s]:  괜찮네.\n",
      "[24.00s - 25.50s]:  on.\n",
      "[25.50s - 27.00s]:  Excuse me. Okay.\n",
      "[27.00s - 28.50s]:  4개 사서 40위 안 들었습니다.\n",
      "[28.50s - 30.00s]: うん\n",
      "[30.00s - 31.50s]:  라인업치 공개는 잔치\n",
      "[31.50s - 33.00s]:  안녕, 압시고.\n",
      "[33.00s - 34.50s]:  또 뭐 브랜드 치고는\n",
      "[34.50s - 36.00s]:  사기 편의\n",
      "[36.00s - 37.50s]:  비싸긴 한데\n",
      "[37.50s - 39.00s]:  If you think of her love\n",
      "[39.00s - 40.50s]: わしにか\n",
      "[40.50s - 42.00s]:  밥은 너무 올려\n",
      "[42.00s - 43.50s]:  다 펜정해서 먹었어.\n",
      "[43.50s - 45.00s]:  거기에 참 살려\n",
      "[45.00s - 46.50s]:  맛있는데 많은데\n",
      "[46.50s - 48.00s]:  너무 멀어.\n",
      "[48.00s - 49.50s]:  Ağakıla.\n",
      "[49.50s - 51.00s]:  갔다만 시작해. 다시 올라오면\n",
      "[51.00s - 52.50s]:  Mmm\n",
      "[52.50s - 54.00s]:  계속\n",
      "[54.00s - 55.50s]:  차스\n",
      "[55.50s - 57.00s]:  아니 그 음식 가볍었나\n",
      "[57.00s - 58.50s]:  여기 있었는데 여기에서\n",
      "[58.50s - 60.00s]:  먹으면 맘박청신 나가도록\n",
      "[60.00s - 61.50s]:  그 정도는 사실 서울이\n",
      "[61.50s - 63.00s]: 多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、多分、\n",
      "[63.00s - 64.50s]:  지연이 성당히 당황하더라고\n",
      "[64.50s - 66.00s]:  야 무슨...\n",
      "[66.00s - 67.50s]: おさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみにおさみに\n",
      "[67.50s - 69.00s]:  전원에 놀래요.\n",
      "[69.00s - 70.50s]:  밥을 심만\n",
      "[70.50s - 72.00s]: 他在演講他\n",
      "[72.00s - 73.50s]:  심학은 이제는 20만원 더 더 예사를 하고\n",
      "[73.50s - 75.00s]:  그런데 자른 마음이\n",
      "[75.00s - 76.50s]:  저의 이내로 생각했으니까\n",
      "[76.50s - 78.00s]:  아...\n",
      "[78.00s - 79.50s]:  하트는 무슨 메뉴가 안 돼?\n",
      "[79.50s - 81.00s]:  전화정도 있는데?\n",
      "[81.00s - 82.50s]:  무슨 늘링냐 하면 돼\n",
      "[82.50s - 84.00s]:  알검청하는데요.\n",
      "[84.00s - 85.50s]:  알았어 재밌게 놀아\n",
      "[85.50s - 87.00s]:  Oh\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 1. Whisper Tiny 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 3. 1.5초 단위 음성 인식 함수\n",
    "def fast_transcription(audio_path, chunk_size=1.5, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    1.5초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 1.5초)\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 1.5초 단위 처리 (chunk_size 초 길이로 나눔)\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 1.5초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 4. 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=1.5, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ad55-dde6-4c76-87d8-a53fb0b104f4",
   "metadata": {},
   "source": [
    "### 한국어만인식 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af141f68-4cc5-445e-ab7a-c88b576fea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 1.50s]:  아\n",
      "[1.50s - 3.00s]:  오늘 여자 올려 부검하냐?\n",
      "[3.00s - 4.50s]:  어\n",
      "[4.50s - 6.00s]:  와우 보고 있냐?\n",
      "[6.00s - 7.50s]:  시간은\n",
      "[7.50s - 9.00s]:  그리고 이곳에서\n",
      "[9.00s - 10.50s]:  랩소라운 매주\n",
      "[10.50s - 12.00s]:  좋은 기침나에서\n",
      "[12.00s - 13.50s]:  음...\n",
      "[13.50s - 15.00s]:  가락이 안 하시\n",
      "[15.00s - 16.50s]:  어마어마한 택시\n",
      "[16.50s - 18.00s]:  랩호\n",
      "[18.00s - 19.50s]:  두 개에서 15만원 샀고\n",
      "[19.50s - 21.00s]:  음\n",
      "[21.00s - 22.50s]:  두 개에서 20천만 원\n",
      "[22.50s - 24.00s]:  괜찮네.\n",
      "[24.00s - 25.50s]:  한 번 뿌려줬어\n",
      "[25.50s - 27.00s]:  오케이\n",
      "[27.00s - 28.50s]:  4개 사서 40위 안 들었습니다.\n",
      "[28.50s - 30.00s]:  좋으니까\n",
      "[30.00s - 31.50s]:  라인업치 공개는 잔치\n",
      "[31.50s - 33.00s]:  안녕, 압시고.\n",
      "[33.00s - 34.50s]:  또 뭐 브랜드 치고는\n",
      "[34.50s - 36.00s]:  사기 편의\n",
      "[36.00s - 37.50s]:  비싸긴 한데\n",
      "[37.50s - 39.00s]:  그런데 비싹을 하려고\n",
      "[39.00s - 40.50s]:  아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아 아, 아 아, 아 아, 아 아, 아 아, 아 아, 아 아, 아 아, 아 아, 아 아 아 아, 아 아 아 아 아 아, 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[40.50s - 42.00s]:  밥은 너무 올려\n",
      "[42.00s - 43.50s]:  다 펜정해서 먹었어.\n",
      "[43.50s - 45.00s]:  거기에 참 살려\n",
      "[45.00s - 46.50s]:  맛있는데 많은데\n",
      "[46.50s - 48.00s]:  너무 멀어.\n",
      "[48.00s - 49.50s]:  아, 그래?\n",
      "[49.50s - 51.00s]:  갔다만 시작해. 다시 올라오면\n",
      "[51.00s - 52.50s]:  음\n",
      "[52.50s - 54.00s]:  계속\n",
      "[54.00s - 55.50s]:  차스\n",
      "[55.50s - 57.00s]:  아니 그 음식 가볍었나\n",
      "[57.00s - 58.50s]:  여기 있었는데 여기에서\n",
      "[58.50s - 60.00s]:  먹으면 맘박청신 나가도록\n",
      "[60.00s - 61.50s]:  그 정도는 사실 서울이\n",
      "[61.50s - 63.00s]:  다 오셨습니다.\n",
      "[63.00s - 64.50s]:  지연이 성당히 당황하더라고\n",
      "[64.50s - 66.00s]:  야 무슨...\n",
      "[66.00s - 67.50s]:  한 번 더 잘 요수하셨으면 좋겠습니다.\n",
      "[67.50s - 69.00s]:  전원에 놀래요.\n",
      "[69.00s - 70.50s]:  밥을 심만\n",
      "[70.50s - 72.00s]:  우선\n",
      "[72.00s - 73.50s]:  심학은 이제는 20만원 더 더 예사를 하고\n",
      "[73.50s - 75.00s]:  그런데 자른 마음이\n",
      "[75.00s - 76.50s]:  저의 이내로 생각했으니까\n",
      "[76.50s - 78.00s]:  아...\n",
      "[78.00s - 79.50s]:  하트는 무슨 메뉴가 안 돼?\n",
      "[79.50s - 81.00s]:  전화정도 있는데?\n",
      "[81.00s - 82.50s]:  무슨 늘링냐 하면 돼\n",
      "[82.50s - 84.00s]:  알검청하는데요.\n",
      "[84.00s - 85.50s]:  알았어 재밌게 놀아\n",
      "[85.50s - 87.00s]:  아허허\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 1.5초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=1.5, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    1.5초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 1.5초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 1.5초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 1.5초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=1.5, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5b93496-2e67-4e3a-aa10-db15b0e4c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 2.00s]:  너네 여자\n",
      "[2.00s - 4.00s]:  아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아 아, 아 아, 아 아, 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[4.00s - 6.00s]:  와우, 뭐고 있냐?\n",
      "[6.00s - 8.00s]:  시간은?\n",
      "[8.00s - 10.00s]:  뭐 이곳적으로 보고\n",
      "[10.00s - 12.00s]:  매종 기칭해서\n",
      "[12.00s - 14.00s]:  음 그..\n",
      "[14.00s - 16.00s]:  커플로 사고\n",
      "[16.00s - 18.00s]:  랜핵\n",
      "[18.00s - 20.00s]:  두 개에서 15만원 샀고.\n",
      "[20.00s - 22.00s]:  매종어 두 개에서 20천만 원\n",
      "[22.00s - 24.00s]:  괜찮네.\n",
      "[24.00s - 26.00s]:  아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아 아, 아 아, 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[26.00s - 28.00s]:  이렇게 퀸 4개에서 40\n",
      "[28.00s - 30.00s]:  이 안 들었으니까\n",
      "[30.00s - 32.00s]:  라인업치 공개는 잔치, 브랜드 라인업치 공개는\n",
      "[32.00s - 34.00s]:  브랜드 브랜드\n",
      "[34.00s - 36.00s]:  근데 치고는 자기 편의해\n",
      "[36.00s - 38.00s]:  비싸긴 한데\n",
      "[38.00s - 40.00s]:  그 기사고 하러 왔으니까\n",
      "[40.00s - 42.00s]:  그래서 밥은 먹었냐?\n",
      "[42.00s - 44.00s]:  다 편점 가서 먹었어.\n",
      "[44.00s - 46.00s]:  거기에 이 채널에 더 맛있는 데 많은데\n",
      "[46.00s - 48.00s]:  거기까지 너무 멀어? 삼아가야 돼\n",
      "[48.00s - 50.00s]:  아 그래?\n",
      "[50.00s - 52.00s]:  이제 이렇게 하시오는\n",
      "[52.00s - 54.00s]:  죄송합니다. 죄송합니다. 죄송합니다. 죄송합니다.\n",
      "[54.00s - 56.00s]:  잘했어\n",
      "[56.00s - 58.00s]:  거의 음식 가볍이 얼마나 있었는데?\n",
      "[58.00s - 60.00s]:  여기서 먹으면 맘빤 청신 나가도록\n",
      "[60.00s - 62.00s]:  아 그 정도는 사실 서울이니까\n",
      "[62.00s - 64.00s]:  형은 진짜 지연의 송농이 다 먹었어\n",
      "[64.00s - 66.00s]:  야, 무슨...\n",
      "[66.00s - 68.00s]:  정말 어차피 옷 사면서 멈탈천동에\n",
      "[68.00s - 70.00s]:  잘 모르겠지?\n",
      "[70.00s - 72.00s]:  심하자리를 예상한 학원에 오셨습니다.\n",
      "[72.00s - 74.00s]:  심학은 이제 20만 정도 예사를 하고 왔는데\n",
      "[74.00s - 76.00s]:  가면 마거쳐 이내로 생각했을니\n",
      "[76.00s - 78.00s]:  아이고\n",
      "[78.00s - 80.00s]:  하트는 무슨 메뉴가 안 발전하는 정도\n",
      "[80.00s - 82.00s]:  네.\n",
      "[82.00s - 84.00s]:  릴링랑은에만큼 처음에 이런 데?\n",
      "[84.00s - 86.00s]:  알았어 재밌게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 2초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    2초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 2초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 2초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 2초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01360f9c-41df-4101-a5dd-3f9dcc6da2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 2.00s]:  너네 여자\n",
      "[2.00s - 4.00s]:  아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아 아, 아 아, 아 아, 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[4.00s - 6.00s]:  와우, 뭐고 있냐?\n",
      "[6.00s - 8.00s]:  시간은?\n",
      "[8.00s - 10.00s]:  뭐 이곳적으로 보고\n",
      "[10.00s - 12.00s]:  매종 기칭해서\n",
      "[12.00s - 14.00s]:  음 그..\n",
      "[14.00s - 16.00s]:  커플로 사고\n",
      "[16.00s - 18.00s]:  랜핵\n",
      "[18.00s - 20.00s]:  두 개에서 15만원 샀고.\n",
      "[20.00s - 22.00s]:  매종어 두 개에서 20천만 원\n",
      "[22.00s - 24.00s]:  괜찮네.\n",
      "[24.00s - 26.00s]:  아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아 아, 아 아, 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아\n",
      "[26.00s - 28.00s]:  이렇게 퀸 4개에서 40\n",
      "[28.00s - 30.00s]:  이 안 들었으니까\n",
      "[30.00s - 32.00s]:  라인업치 공개는 잔치, 브랜드 라인업치 공개는\n",
      "[32.00s - 34.00s]:  브랜드 브랜드\n",
      "[34.00s - 36.00s]:  근데 치고는 자기 편의해\n",
      "[36.00s - 38.00s]:  비싸긴 한데\n",
      "[38.00s - 40.00s]:  그 기사고 하러 왔으니까\n",
      "[40.00s - 42.00s]:  그래서 밥은 먹었냐?\n",
      "[42.00s - 44.00s]:  다 편점 가서 먹었어.\n",
      "[44.00s - 46.00s]:  거기에 이 채널에 더 맛있는 데 많은데\n",
      "[46.00s - 48.00s]:  거기까지 너무 멀어? 삼아가야 돼\n",
      "[48.00s - 50.00s]:  아 그래?\n",
      "[50.00s - 52.00s]:  이제 이렇게 하시오는\n",
      "[52.00s - 54.00s]:  죄송합니다. 죄송합니다. 죄송합니다. 죄송합니다.\n",
      "[54.00s - 56.00s]:  잘했어\n",
      "[56.00s - 58.00s]:  거의 음식 가볍이 얼마나 있었는데?\n",
      "[58.00s - 60.00s]:  여기서 먹으면 맘빤 청신 나가도록\n",
      "[60.00s - 62.00s]:  아 그 정도는 사실 서울이니까\n",
      "[62.00s - 64.00s]:  형은 진짜 지연의 송농이 다 먹었어\n",
      "[64.00s - 66.00s]:  야, 무슨...\n",
      "[66.00s - 68.00s]:  정말 어차피 옷 사면서 멈탈천동에\n",
      "[68.00s - 70.00s]:  잘 모르겠지?\n",
      "[70.00s - 72.00s]:  심하자리를 예상한 학원에 오셨습니다.\n",
      "[72.00s - 74.00s]:  심학은 이제 20만 정도 예사를 하고 왔는데\n",
      "[74.00s - 76.00s]:  가면 마거쳐 이내로 생각했을니\n",
      "[76.00s - 78.00s]:  아이고\n",
      "[78.00s - 80.00s]:  하트는 무슨 메뉴가 안 발전하는 정도\n",
      "[80.00s - 82.00s]:  네.\n",
      "[82.00s - 84.00s]:  릴링랑은에만큼 처음에 이런 데?\n",
      "[84.00s - 86.00s]:  알았어 재밌게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 2초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    2초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 2초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 2초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 2초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16999690-73ac-48c0-a1e4-9d331a79d8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe96fad979447a1b3e313e2f4903999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6721cb2172a8403b8b1ef67a8351701a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94096a1a15a46b989f1b2098476bc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c384f3dc7ced41fa8144704a65c01378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cac46bb3d64794ad82eea18b1064fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c37665a3ec423fbf7ea56b101b29bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f701ff27d74a28a8f37c920a33cac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b85e44182904a33a5beb6cbbd68eb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5f62f3636b403b83c188cfb92c3781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811599e1b23d41bc8965275b91ab1fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7e5563f339493b957dfa7216d8193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 2.00s]:  어\n",
      "[2.00s - 4.00s]:  나 원래 구경하냐? 어\n",
      "[4.00s - 6.00s]:  뭐하고 뭐 보고 있냐 지금\n",
      "[6.00s - 8.00s]:  지금은? 지금.\n",
      "[8.00s - 10.00s]:  뭐 이것저것 보고\n",
      "[10.00s - 12.00s]:  맵고랑 매종키친에서\n",
      "[12.00s - 14.00s]:  그 뭐야 카라\n",
      "[14.00s - 16.00s]:  어떻게 하나씩 커플로 사고\n",
      "[16.00s - 18.00s]:  락쿠\n",
      "[18.00s - 20.00s]:  2개에서 15만원 샀고\n",
      "[20.00s - 22.00s]:  매종 2개에서 23만원\n",
      "[22.00s - 24.00s]:  괜찮네 2개는 2,3이면\n",
      "[24.00s - 26.00s]:  아, 그래서...\n",
      "[26.00s - 28.00s]:  이렇게 총 4개 사서 40...\n",
      "[28.00s - 30.00s]:  이 안 들었으니까\n",
      "[30.00s - 32.00s]:  라인업 치고 괜찮지? 그런데 라인업 치고\n",
      "[32.00s - 34.00s]:  브랜드..뭐..\n",
      "[34.00s - 36.00s]:  근데 치고는 자기 맞네\n",
      "[36.00s - 38.00s]:  비싸긴 한데\n",
      "[38.00s - 40.00s]:  비슷한거 하고 왔으니까\n",
      "[40.00s - 42.00s]:  그래 알았어 밥은 뭐 먹었냐\n",
      "[42.00s - 44.00s]:  다 편의점 가서 먹었어\n",
      "[44.00s - 46.00s]:  거기 2000살 맛있나봐\n",
      "[46.00s - 48.00s]:  거기까지 너무 멀어 삼슨 가야 돼\n",
      "[48.00s - 50.00s]:  아 그래?\n",
      "[50.00s - 52.00s]:  다시 올라오면\n",
      "[52.00s - 54.00s]:  그래서\n",
      "[54.00s - 56.00s]:  진짜 잘했어\n",
      "[56.00s - 58.00s]:  음식값이 얼마나 비쌌는데?\n",
      "[58.00s - 60.00s]:  여기서 먹으면 인낙은 만발청식 나가더라고\n",
      "[60.00s - 62.00s]:  아 그 정도는 하지 서울이니까\n",
      "[62.00s - 64.00s]:  응. 탕후루스 지혜는 상당히 탕후루어다.\n",
      "[64.00s - 66.00s]:  나 무서워\n",
      "[66.00s - 68.00s]:  100만원짜리 옷 사면서 18000원에\n",
      "[68.00s - 70.00s]:  놀래 웃겨 죽겠네\n",
      "[70.00s - 72.00s]:  10만원짜리를 예상하고\n",
      "[72.00s - 74.00s]:  100만원 정도 예상을 하고 왔는데\n",
      "[74.00s - 76.00s]:  답은 15,000원 이내로 생각했으니까\n",
      "[76.00s - 78.00s]:  그니까\n",
      "[78.00s - 80.00s]:  무슨 메뉴가 1,800원 정도야\n",
      "[80.00s - 82.00s]:  또 했는데.\n",
      "[82.00s - 84.00s]:  1냉면 한에 13000원이던데?\n",
      "[84.00s - 86.00s]:  알았어 재밌게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-small\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 2초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    2초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 2초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 2초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 2초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13fc8093-ba79-4995-ade2-29d1241549d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 3.00s]:  너네 여자 원래 부검하냐?\n",
      "[3.00s - 6.00s]:  와우, 뭐고 있냐?\n",
      "[6.00s - 9.00s]:  지금 뭐 이곳적으로 보고\n",
      "[9.00s - 12.00s]:  라토랑 매종 기칭에서\n",
      "[12.00s - 15.00s]:  그...\n",
      "[15.00s - 18.00s]:  엄만드\n",
      "[18.00s - 21.00s]:  2개에서 15만원 샀고\n",
      "[21.00s - 24.00s]:  2개에서 2000만 원.\n",
      "[24.00s - 27.00s]:  아, 프리스 오, 그 is, 이렇게\n",
      "[27.00s - 30.00s]:  4개에서 40개 안 들었으니까.\n",
      "[30.00s - 33.00s]:  라인 없이 공개는 괜찮지? 그런 데 라인 없이 공개\n",
      "[33.00s - 36.00s]:  뭐 브랜드 치고는 자기 다네\n",
      "[36.00s - 39.00s]:  비싸긴 한데 비싸고 하려고\n",
      "[39.00s - 42.00s]:  아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우 아우\n",
      "[42.00s - 45.00s]:  다 편제먹어서 먹었어. 거기에 이 챤살 먹었어.\n",
      "[45.00s - 48.00s]:  맛있는데 많은데? 거기까지 너무 멀어 삼아진 거야\n",
      "[48.00s - 51.00s]:  아 그래? 응. 갔다만 시작해야 돼. 다시 올라오면.\n",
      "[51.00s - 54.00s]:  죄송합니다\n",
      "[54.00s - 57.00s]:  진짜 맛있어\n",
      "[57.00s - 60.00s]:  여기에서 먹으면 맘빡청신 나가도라고\n",
      "[60.00s - 63.00s]:  그 정도는 사실 서울이니까\n",
      "[63.00s - 66.00s]:  지연이 형이 당황하더라고\n",
      "[66.00s - 69.00s]:  정말 어차피 우수하면서 멈탈 전원에 놀래요.\n",
      "[69.00s - 72.00s]:  밥은 신모자를 예상한다고\n",
      "[72.00s - 75.00s]:  심학은 제 20만원에 의사를 하고 왔는데\n",
      "[75.00s - 78.00s]:  저의 인해로 생각했으니까\n",
      "[78.00s - 81.00s]:  하트인은 무슨 메뉴가 안 발전하는 정도에 있는데?\n",
      "[81.00s - 84.00s]:  무슨 물냉량을 많이 참쳐서 이런데?\n",
      "[84.00s - 87.00s]:  알았어 재미있게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 3초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=3.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    3초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 3초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 3초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 3초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=3.0, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8653db0-cc3f-4368-b764-6c62b7bc45f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 4.00s]:  너네 여자 원래 부검하냐?\n",
      "[4.00s - 8.00s]:  와우 보고 있냐 지금은\n",
      "[8.00s - 12.00s]:  뭐 이것저것 보고 라투랑 매종 기칭해서\n",
      "[12.00s - 16.00s]:  그 카라티 하나씩 커플로 사고\n",
      "[16.00s - 20.00s]:  랩호우는 두 개에서 15만원 샀고\n",
      "[20.00s - 24.00s]:  매종어 두 개에서 20천만 원.\n",
      "[24.00s - 28.00s]:  그래서 이렇게 형 4개 사서 40\n",
      "[28.00s - 32.00s]:  이 안 들었으니까 라인업치고 괜찮지? 그런 데 라인업치고\n",
      "[32.00s - 36.00s]:  브랜드 치고는 자기 다네\n",
      "[36.00s - 40.00s]:  좀 비싸긴 한데, 비싸고 하러 왔으니까.\n",
      "[40.00s - 44.00s]:  그래 알았어 밥은 먹느냐? 다 편집어서 먹었어\n",
      "[44.00s - 48.00s]:  거기에 참 살고 맛있는데 많은데? 거기에 너무 멀어 삼아간 거야\n",
      "[48.00s - 52.00s]:  아 그래? 응. 갔다만씩 해야 돼. 이렇게 다시 올라오면.\n",
      "[52.00s - 56.00s]:  그래서 좀 비찾았어?\n",
      "[56.00s - 60.00s]:  여기에서 먹으면 맘발청신 나가도라고\n",
      "[60.00s - 64.00s]:  그 정도는 사실 서울이니까\n",
      "[64.00s - 68.00s]:  야 무슨 신만 어차피 웃으면서 멈탈 천 원에\n",
      "[68.00s - 72.00s]:  밥은 심망자를 예상한다고\n",
      "[72.00s - 76.00s]:  심학은 자연이 20만원에 의사를 하고 왔는데 가면 많아 쳐다리 이내로 생각했을니\n",
      "[76.00s - 80.00s]:  아유 원래 그 정도 하지는 무슨 메뉴가 안 발전하는 정도\n",
      "[80.00s - 84.00s]:  무슨 물냉량을 많이 참쳐서 이런데?\n",
      "[84.00s - 87.54s]:  알았어 재미있게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 4초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=4.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    4초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 4초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 4초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 4초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks + 1):  # 마지막 청크까지 포함\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        \n",
    "        # 마지막 청크 처리 (길이가 chunk_length보다 짧을 수 있음)\n",
    "        if start >= len(audio):\n",
    "            break\n",
    "        if end > len(audio):\n",
    "            end = len(audio)\n",
    "        \n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=4.0, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d803af-2e80-4a44-9ef2-0efc323cdf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 4.00s]:  너네 여주아울래 구경하냐?\n",
      "[4.00s - 8.00s]:  뭐하고 뭐 보고 있냐 지금은? 지금\n",
      "[8.00s - 12.00s]:  뭐 이것저것 보고 라코랑 매종키친에서\n",
      "[12.00s - 16.00s]:  그 뭐야 카라티 하나씩 커플로 사고 얼마\n",
      "[16.00s - 20.00s]:  락코는 2개에서 15만원 샀고\n",
      "[20.00s - 24.00s]:  매종은 2개에서 23만원\n",
      "[24.00s - 28.00s]:  그래서 총 4개 사서 40\n",
      "[28.00s - 32.00s]:  이 안 들었으니까 라인업 치고 괜찮지? 그런데 라인업 치고\n",
      "[32.00s - 36.00s]:  브랜드 치고는 자기 맞네\n",
      "[36.00s - 40.00s]:  비싸긴 한데 비싸고 가고 왔으니까\n",
      "[40.00s - 44.00s]:  그래 알았어 밥은 뭐 먹었냐? 밥 편집만 가서 먹었어\n",
      "[44.00s - 48.00s]:  거기 2000살 밥 맛있는 데 많은데 거기까지 너무 멀어 30분 가야 돼\n",
      "[48.00s - 52.00s]:  아 그래? 응. 갔다 오면 1시간이 됐다. 다시 올라가.\n",
      "[52.00s - 56.00s]:  그래서 좀 비찼었어\n",
      "[56.00s - 60.00s]:  그냥 거기 음식값이 얼마나 비쌌는데? 여기서 먹으면 인낙한 만발청식 나가더라고\n",
      "[60.00s - 64.00s]:  아 그 정도는 하지 서울이니까\n",
      "[64.00s - 68.00s]:  야 무슨.. 10만원짜리 옷 사면서 18000원에\n",
      "[68.00s - 72.00s]:  놀래 웃겨 죽겠네\n",
      "[72.00s - 76.00s]:  10만원 정도 예상을 하고 왔는데 답은 15,000원 이내로 생각했으니까\n",
      "[76.00s - 80.00s]:  그 정도 하지 무슨 메뉴가 안 8000원 정도야\n",
      "[80.00s - 84.00s]:  했는데 무슨 물 냉면 한에 13,000원이던데?\n",
      "[84.00s - 87.54s]:  알았어 재밌게 놀아\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# 모델 로드\n",
    "model_name = \"openai/whisper-small\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 무음인지 판단하는 함수\n",
    "def is_silent(audio_chunk, threshold=0.01):\n",
    "    \"\"\"\n",
    "    오디오 청크가 무음인지 확인\n",
    "    - threshold: 무음으로 간주할 최대 진폭 값 (기본값 0.01)\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_chunk)) < threshold\n",
    "\n",
    "# 4초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=7.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    4초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 4초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 4초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 4초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks + 1):  # 마지막 청크까지 포함\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        \n",
    "        # 마지막 청크 처리 (길이가 chunk_length보다 짧을 수 있음)\n",
    "        if start >= len(audio):\n",
    "            break\n",
    "        if end > len(audio):\n",
    "            end = len(audio)\n",
    "        \n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 무음 청크 건너뛰기\n",
    "        if is_silent(audio_chunk):\n",
    "            continue\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=4.0, sampling_rate=16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd55c7-f009-4a68-a5bf-e9dd7ab2ecc8",
   "metadata": {},
   "source": [
    "### ko speech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78cb5250-c58b-4df8-a321-bee00e024310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축이 풀렸습니다: kospeech-latest\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"kospeech-latest.zip\"\n",
    "extract_path = \"kospeech-latest\"\n",
    "\n",
    "# 압축 풀기\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"압축이 풀렸습니다: {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43c86ee7-b562-4718-b09a-cef18f73d75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KoSpeechModel' from 'kospeech.kospeech' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkospeech\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkospeech\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KoSpeechModel\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KoSpeechModel' from 'kospeech.kospeech' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from kospeech.kospeech import KoSpeechModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10c5c9-811e-4f2d-80b5-0da098393d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = KoSpeechModel(\"kospeech-latest\")\n",
    "\n",
    "# 모델 평가 모드\n",
    "model.eval()\n",
    "\n",
    "# 2초 단위 음성 인식 함수 (한국어만 인식)\n",
    "def fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    2초 단위로 오디오 데이터를 처리하여 빠른 텍스트 출력\n",
    "    - chunk_size: 처리할 오디오 길이 (초 단위, 기본값 2초)\n",
    "    - 한국어만 인식되도록 강제 설정\n",
    "    \"\"\"\n",
    "    # 오디오 파일 로드\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # 2초 단위 처리\n",
    "    chunk_length = int(chunk_size * sampling_rate)  # 2초에 해당하는 샘플 수\n",
    "    total_chunks = len(audio) // chunk_length  # 총 처리할 청크 수\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        # 현재 청크 추출\n",
    "        start = i * chunk_length\n",
    "        end = start + chunk_length\n",
    "        audio_chunk = audio[start:end]\n",
    "        \n",
    "        # 입력 데이터 준비\n",
    "        input_features = model.processor(audio_chunk, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # 모델 추론 (한국어 강제 설정)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                forced_decoder_ids=model.processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "            )\n",
    "        \n",
    "        # 결과 디코딩\n",
    "        transcription = model.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # 결과 출력\n",
    "        print(f\"[{start / sampling_rate:.2f}s - {end / sampling_rate:.2f}s]: {transcription}\")\n",
    "\n",
    "# 실행\n",
    "audio_path = \"data1.wav\"  # 음원 파일 경로\n",
    "fast_transcription(audio_path, chunk_size=2.0, sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2087b31-5044-4e72-9bc2-bb93cf881944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
